---
title: "Entregable1"
author: "Norberto García Marín y María Soledad Pérez López"
date: "November 29, 2018"
output: pdf_document
---

# Índice


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Carga de la base de datos

```{r setup, include=FALSE}

credit <-read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data", sep = ",", na.strings = "?", header = F)
colnames(credit) <- c("A1","A2","A3","A4","A5","A6","A7","A8","A9","A10","A11","A12","A13","A14","A15","A16")


credit.trainIdx<-readRDS("credit.trainIdx.rds")
credit.Datos.Train<-credit[credit.trainIdx,]
credit.Datos.Test<-credit[-credit.trainIdx,]

nrow(credit.Datos.Train)
nrow(credit.Datos.Test)
```

# Preprocesar Datos
## Analizar las variables (análisis monovariable y multivariables)

```{r setup, include=FALSE}

# Resumen de los datos

summary(credit.Datos.Train)

summary(credit.Datos.Test)

summary(credit)


```


## Tratar valores nulos/externos/atípicos/desconocidos si los hubiese

## Eliminar variables superfluas/añadir variables nuevas

## Transformar justificadamente los datos (normalizar/escalar/etc)




# Entrenar varios Modelos/Técnicas

## Escoger varios modelos/técnicas a comparar

## Encontrar los mejores hiper-parámetros (usar TunerGrid en, al menos, 1 modelo)

## Probar, al menos, un mismo modelo/técnica con 2 preprocesos de datos diferentes



# Comparar Modelos

## Comparar el rendimiento de los diferentes modelos

## Decidir justificadamente el modelo final

## Evaluar el rendimiento futuro del modelo final